---
title: "Predicting Car Crashes Caused by Alcohol Using Logistic Regression"
author: "Emily Zhou, Ziyi Guo, Emma Jiang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: simplex
    toc: yes
    toc_float: yes
    code_folding: hide
    code_download: yes

editor_options:
  markdown:
    wrap: sentence
---

Version 1.0 | First Created Nov 4, 2024 | Updated Nov 6, 2024

Keywords: t-test, chi-square, multiple logistic regression, ROC, sensitivity, specificity, odds ratio

GitHub Repository: [CPLN671-Logit-Regression](https://github.com/emilyzhou112/CPLN671-Logit-Regression)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages, message=FALSE, warning=FALSE, include=FALSE}

options(scipen=999)
options(digits = 3)

packages <- c("tidyverse", "sf", "here", "ggplot2", "kableExtra", "patchwork", "gmodels", "ggcorrplot", "ROCR", "plotROC")

package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, quietly=TRUE)
      library(x, character.only = TRUE)
    }
  }
)

```


```{r load data, message=FALSE, warning=FALSE, include=FALSE}

crashes <- read.csv(here("data", "LogisticRegressionData.csv"))

```


# Introduction

State the problem, the importance of the problem, and the setting of the analysis (Philadelphia)
Speculate as to why the predictors weâ€™re using might be associated with the response variable.
Indicate that you will be using R to run logistic regression for the analysis.


# Methods

a) Explain the problems with using OLS regression when the dependent variable is binary

b) Explain how logistic regression works around these issues.
Prior to diving into logistic regression, introduce the concept of odds. Explain what an odds ratio is, and how it may be interpreted.
Present the regression equation for the logit model with multiple predictors. Present the regression equation in both the logit form and the logistic form. In the formulas in (a) above, instead of writing ð‘Œ, ð‘¥1â€¦ ð‘¥ð‘˜, etc., write the names of the dependent and independent variables used in our actual model. Be sure to explain every term in each equation and talk about the logit and logistic functions (feel free to use the graphs in the slides as part of your explanation). Specifically, talk about the properties of the logistic function and explain why the logistic function works well for models where the dependent variable is binary.

c) Describe the hypothesis that is tested for each predictor
State the null and alternative hypotheses
Talk about the Wald statistic and the distribution that it follows
State that rather than looking at the estimated ð›½ coefficients, most statisticians prefer to look at odds ratios, which are calculated by exponentiating the coefficients.

d) Talk about how you assess the quality of model fit.
Mention that an R-squared may be calculated for logistic regression, however it is no longer a very useful metric and doesnâ€™t have the same interpretation as in OLS.
Talk about the Akaike Information Criterion to compare models. In one sentence describe what the AIC is (if the information in the slides isnâ€™t sufficient, I suggest Googling it or looking at Wikipedia), and state whether a lower AIC is indicative of a better or worse fit.
Explain what is meant by specificity, sensitivity and the misclassification rate, and describe how each quantity is calculated. Are higher or lower values of each quantity better or worse? In your explanation, be sure to mention about how fitted (predicted) values of ð‘¦, i.e., ð‘¦are calculated and interpreted in logistic regression. Indicate why you should try using different cut-offs for what is considered a â€œhighâ€ probability of ð‘Œ = 1 when calculating the specificity, sensitivity and the misclassification rate. Explain what the ROC curve is. Be sure to talk about some
methods for calculating the optimal cut-off using the ROC curve, and specify which one you will be using in this report. Also, explain what we get by calculating the area under the ROC curve, and what might be value ranges for excellent, good, fair, poor and failing models.

e) Talk about the assumptions of logistic regression. Which assumptions of OLS regression also hold for logistic regression, and which donâ€™t?


f) Describe the exploratory analyses that statisticians may want to do before running logistic regression.
Talk about running the cross-tabulations between the dependent variable and binary predictors to see whether there is an association between the two variables. Say that the appropriate statistical test for examining the
association between two categorical variables is the Chi-Square test. Be sure to mention the null and alternative hypotheses for the test.

Also state that we can compare the means of continuous predictors for both values of the dependent variable. Say that the independent samples t-tests are the appropriate statistical tests for examining whether there were significant differences in mean values of PCTBACHMOR and MEDHHINC for crashes that involved alcohol and those that didnâ€™t. Mention the null and alternative hypotheses for the t-test.


# Results 

## Exploratory Analysis

Present the tabulation of the dependent variable and comment on the number and proportion of crashes that involves drunk driving.

```{r}

table(crashes$DRINKING_D)
depetab <- prop.table(table(crashes$DRINKING_D))
deptab <- as.data.frame(depetab)
colnames(deptab) <- c("DRINKING_D", "Proportion")
deptab %>% 
  kable("html", escape = FALSE, align = "cc") %>%
  kable_styling(full_width = FALSE, position = "center", fixed_thead = TRUE)

```

Present the cross-tabulation of the dependent variable with each of the binary predictors (table on p. 4 above).


```{r crosstabulation, message=FALSE, warning=FALSE, echo=TRUE, results='hide'}

extract_crosstab_values <- function(var1, var2) {

  crosstab <- CrossTable(var1, var2, prop.r = FALSE, prop.chisq = FALSE, chisq = FALSE, prop.t = FALSE)
  
  value_1 <- crosstab$t["1", "1"]                                   # Count where DRINKING_D = 1, other variable = 1
  value_2 <- (crosstab$t["1", "1"] / sum(crosstab$t["1",])) * 100   # Pct where DRINKING_D = 1, other variable = 1
  value_3 <- crosstab$t["0", "1"]                                   # Count where DRINKING_D = 0, other variable = 1
  value_4 <- (crosstab$t["0", "1"] / sum(crosstab$t["0",])) * 100   # Pct where DRINKING_D = 0, other variable = 1
  value_5 <- crosstab$t["1", "1"] + crosstab$t["0", "1"]            # Total count where other variable = 1

  return(c(Value1 = value_1, Value2 = value_2, Value3 = value_3, Value4 = value_4, Value5 = value_5))
}

tabulation <- data.frame(
  Variable = character(),
  Value1 = numeric(),
  Value2 = numeric(),
  Value3 = numeric(),
  Value4 = numeric(),
  Value5 = numeric()
)

descriptions <- c(
  FATAL_OR_M = "Crash resulted in fatality or major injury",
  OVERTURNED = "Crash involved an overturned vehicle",
  CELL_PHONE = "Driver was using cell phone",
  SPEEDING = "Crash involved speeding car",
  AGGRESSIVE = "Crash involved aggressive driving",
  DRIVER1617 = "Crash involved at least one driver 16 or 17 years old",
  DRIVER65PLUS = "Crash involved at least one driver over 65 years old"
)

variables <- c("FATAL_OR_M", "OVERTURNED", "CELL_PHONE", "SPEEDING", "AGGRESSIVE", "DRIVER1617", "DRIVER65PLUS")

for (var in variables) {
  values <- extract_crosstab_values(crashes$DRINKING_D, crashes[[var]])
  
  tabulation <- rbind(
    tabulation,
    data.frame(Variable = paste0(var, ": ", descriptions[[var]]), t(values))
  )
}

colnames(tabulation) <- c("", "N", "%", "N ", "% ", "N  ")

```



```{r}
tabulation %>%
  kable("html", escape = FALSE, align = "lccccc") %>%
  kable_styling(full_width = FALSE, position = "center", fixed_thead = TRUE) %>%
  column_spec(1, width = "30em") %>%    # Wider first column for descriptions
  column_spec(2, width = "5em") %>%
  column_spec(3, width = "5em") %>%
  column_spec(4, width = "5em") %>%
  column_spec(5, width = "5em") %>%
  column_spec(6, width = "5em") %>%
  add_header_above(c(" " = 1, "DRINKING_D = 0" = 2, "DRINKING_D = 1" = 2, " " = 1)) %>% 
  add_header_above(c(" " = 1, "No Alcohol" = 2, "Alcohol Involved" = 2, "Total" = 1))
```

In the table, add (an) extra column(s) which presents the results of the Chi-Square test (you may present the p-value only, or the Chi-Square statistic, the degrees of freedom and the p-value).
Discuss whether the Chi-Square test shows that there is a significant association between the dependent variable and each of the binary predictors (i.e., can you reject the Null Hypothesis?)

```{r}

tabulation$Chi_squared <- NA  
tabulation$p_value <- NA

for (i in seq_along(variables)) {
  var <- variables[i]
  chi_test <- chisq.test(table(crashes$DRINKING_D, crashes[[var]]), correct = FALSE)
  tabulation$Chi_squared[i] <- round(chi_test$statistic, 2)
  tabulation$p_value[i] <- round(chi_test$p.value, 5)
}

tabulation %>%
  kable("html", escape = FALSE, align = "lcccccc") %>%
  kable_styling(full_width = FALSE, position = "center", fixed_thead = TRUE) %>%
  column_spec(1, width = "50em") %>%
  column_spec(2, width = "5em") %>%
  column_spec(3, width = "5em") %>%
  column_spec(4, width = "5em") %>%
  column_spec(5, width = "5em") %>%
  column_spec(6, width = "5em") %>%
  column_spec(7, width = "5em") %>%
  column_spec(8, width = "5em") %>%
  add_header_above(c(" " = 1, "DRINKING_D = 0" = 2, "DRINKING_D = 1" = 2, " " = 1, " " = 2)) %>% 
  add_header_above(c(" " = 1, "No Alcohol" = 2, "Alcohol Involved" = 2, "Total" = 1, "Ï‡2 Test" = 2))
```

Present the means of the continuous predictors for both values of the dependent variable (table on p. 5).

```{r}

mean_pctbachmor <- tapply(crashes$PCTBACHMOR, crashes$DRINKING_D, mean)
sd_pctbachmor <- tapply(crashes$PCTBACHMOR, crashes$DRINKING_D, sd)
mean_medhhinc <- tapply(crashes$MEDHHINC, crashes$DRINKING_D, mean)
sd_medhhinc <- tapply(crashes$MEDHHINC, crashes$DRINKING_D, sd)

tabulation_cont <- data.frame(
  Variable = c("PCTBACHMOR: % with bachelorâ€™s degree or more", "MEDHHINC: Median household income"),
  Value1 = c(mean_pctbachmor["0"], mean_medhhinc["0"]),  # Mean for DRINKING_D = 0
  Value2 = c(sd_pctbachmor["0"], sd_medhhinc["0"]),       # SD for DRINKING_D = 0
  Value3 = c(mean_pctbachmor["1"], mean_medhhinc["1"]),  # Mean for DRINKING_D = 1
  Value4 = c(sd_pctbachmor["1"], sd_medhhinc["1"])        # SD for DRINKING_D = 1
)

colnames(tabulation_cont) <- c("", "Mean", "SD", "Mean ", "SD ")

tabulation_cont %>%
  kable("html", escape = FALSE, align = "lcccccc") %>%
  kable_styling(full_width = FALSE, position = "center", fixed_thead = TRUE) %>%
  add_header_above(c(" " = 1, "DRINKING_D = 0" = 2, "DRINKING_D = 1" = 2)) %>% 
  add_header_above(c(" " = 1, "No Alcohol" = 2, "Alcohol Involved" = 2))
```


In the table, add an extra column which presents the results of the independent samples t-test (you may present the p-value only, or the t-statistic, the degrees of freedom and the p-value).
Discuss whether the t-test shows that there is a significant association between the dependent variable and each of the
continuous predictors (i.e., can you reject the Null Hypothesis?).

```{r}

pval_pctbachmor <- t.test(crashes$PCTBACHMOR ~ crashes$DRINKING_D)$p.value
pval_medhhinc <- t.test(crashes$MEDHHINC ~ crashes$DRINKING_D)$p.value
tabulation_cont$`t-test p-value` <- c(pval_pctbachmor, pval_medhhinc)  

tabulation_cont %>%
  kable("html", escape = FALSE, align = "lcccccc") %>%
  kable_styling(full_width = FALSE, position = "center", fixed_thead = TRUE) %>%
  add_header_above(c(" " = 1, "DRINKING_D = 0" = 2, "DRINKING_D = 1" = 2, " " =  1)) %>% 
  add_header_above(c(" " = 1, "No Alcohol" = 2, "Alcohol Involved" = 2, " " = 1))
```

## Regression Assumptions

Comment on which logistic regression assumptions are met and which ones are violated for the problem at hand.
In particular, be sure to present the matrix showing the pairwise Pearson correlations for all the binary and continuous predictors.Comment on any potential limitations of using Pearson correlations to measure the associations between binary predictors. State whether there is evidence of multicollinearity, and remind the reader how youâ€™re defining multicollinearity.

```{r fig.height=7, fig.width=7}

cor_matrix <- cor(crashes %>% dplyr::select(-c(CRN, DRINKING_D, AREAKEY, COLLISION_)), use = "pairwise.complete.obs", method = "pearson")


custom_label <- c(
  "Crash resulted in fatality or major injury" = "FATAL_OR_M",
  "Crash involved an overturned vehicle" = "OVERTURNED",
  "Driver was using cell phone" = "CELL_PHONE",
  "Crash involved speeding car" = "SPEEDING",
  "Crash involved aggressive driving" = "AGGRESSIVE",
  "Crash involved at least one driver 16 or 17 years old" = "DRIVER1617",
  "Crash involved at least one driver over 65 years old" = "DRIVER65PLUS",
  "% with bachelorâ€™s degree" = "PCTBACHMOR",
  "Median household income" = "MEDHHINC"
)


rownames(cor_matrix) <- names(custom_label)
colnames(cor_matrix) <- names(custom_label)

ggcorrplot(cor_matrix, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           colors = c("#283d3b", "white", "#c44536")) +
  labs(title = "Correlation Matrix for all Predictor Variables") +
  theme(plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7), 
        axis.title = element_text(size = 8))
```

## Regression Analysis

First, present the results of the logistic regression with all predictors. Interpret the results. Be sure to comment on whether each predictor is significant, and interpret the odds ratio for each predictor


```{r}

logit <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS + PCTBACHMOR + MEDHHINC, data = crashes, family = binomial)

summary(logit)

```


```{r report result, warning=FALSE, message=FALSE}

logitoutput <- summary(logit)
logitcoeff <- logitoutput$coefficients
or_ci <- exp(cbind(OR = coef(logit), confint(logit)))
logitcoeff[, "Pr(>|z|)"] <- round(logitcoeff[, "Pr(>|z|)"], 4)
final_output <- cbind(logitcoeff, or_ci)
final_output

```
Present the specificity, sensitivity and the misclassification rates for the different probability cut-offs. Indicate the cut-offs which yield the lowest/highest misclassification rates.

```{r}
a <- cbind(crashes$DRINKING_D, fit)
colnames(a) <- c("labels", "predictions")
roc <- as.data.frame(a)
```


```{r}

cutoffs <- c(0.02, 0.03, 0.05, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.50)
results <- data.frame(
  Cutoff = cutoffs,
  Sensitivity = NA,
  Specificity = NA,
  MisclassificationRate = NA
)

calculate_metrics <- function(cutoff, labels, predictions) {
  predicted_labels <- ifelse(predictions >= cutoff, 1, 0)
  
  tp <- sum(predicted_labels == 1 & labels == 1) # True positives
  tn <- sum(predicted_labels == 0 & labels == 0) # True negatives
  fp <- sum(predicted_labels == 1 & labels == 0) # False positives
  fn <- sum(predicted_labels == 0 & labels == 1) # False negatives
  
  # Sensitivity: True positive rate
  sensitivity <- tp / (tp + fn)
  
  # Specificity: True negative rate
  specificity <- tn / (tn + fp)
  
  # Misclassification rate
  misclassification_rate <- (fp + fn) / length(labels)
  
  return(c(sensitivity, specificity, misclassification_rate))
}

for (i in seq_along(cutoffs)) {
  metrics <- calculate_metrics(cutoffs[i], roc$labels, roc$predictions)
  results$Sensitivity[i] <- metrics[1]
  results$Specificity[i] <- metrics[2]
  results$MisclassificationRate[i] <- metrics[3]
}

results
```



```{r}
best_cutoff <- results$Cutoff[which.min(results$MisclassificationRate)]
best_cutoff
```


Present the ROC curve and comment on the optimal cut-off rate that was selected by minimizing the distance from the upper left corner of the ROC curve.
Compare this cut-off rate with the optimal rate in 3.c.ii above. Keep in mind that in 3.c.ii, weâ€™re looking at minimum mis-classification rates, and here, weâ€™re looking at simultaneously minimizing both sensitivity and specificity.

```{r}

ggplot(roc, aes(d = labels, m = predictions)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#283d3b") +
  labs(title = "ROC Curve") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1, color = "#c44536") +
  theme(
    plot.subtitle = element_text(size = 9, face = "italic"),
    plot.title = element_text(size = 12, face = "bold"), 
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8), 
    axis.title = element_text(size = 9), 
    panel.background = element_blank(),
    panel.border = element_rect(colour = "grey", fill = NA, linewidth = 0.8)
  )
```


```{r}

pred <- prediction(roc$predictions, roc$labels)
roc.perf = performance(pred, measure = "tpr", x.measure="fpr")

opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(roc.perf, pred))
```
Also present and comment on the area under the ROC curve. What does it tell us about our model?

```{r}
auc.perf = performance(pred, measure ="auc")
auc.perf@y.values
```

Finally, present the results of the logistic regression with the binary predictors only (i.e., without PCTBACHMOR and MEDHHINC). Compare the results of this regression with the results of the first regression: are there any predictors which are significant in the new model which werenâ€™t significant in the original one, or vice versa?
Be sure to also present the Akaike Information Criterion (AIC) for both models and indicate which model is better.

```{r}

logit2 <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS, data = crashes, family = binomial)

summary(logit2)

```

```{r report result, warning=FALSE, message=FALSE}

logitoutput2 <- summary(logit2)
logitcoeff2 <- logitoutput2$coefficients
or_ci2 <- exp(cbind(OR = coef(logit2), confint(logit2)))
logitcoeff2[, "Pr(>|z|)"] <- round(logitcoeff2[, "Pr(>|z|)"], 4)
final_output2 <- cbind(logitcoeff2, or_ci2)
final_output2

```

```{r}
AIC(logit, logit2)
```

# Discussion
